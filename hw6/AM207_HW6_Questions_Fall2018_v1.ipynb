{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "##### Data: HW6_data.csv\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Saturday, October 20th, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "** Place the name of everyone who's submitting this assignment here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Can I sample from F-R-I-E-N-D-S without rejection?  It's Important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding required**\n",
    "\n",
    "\n",
    "*Haven't we made it obvious?* In HW 5 we were introduced to $X$ a random variable with distribution described by the following pdf:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\begin{cases}\n",
    "\\frac{1}{12}(x-1), &1\\leq x\\leq 3\\\\\n",
    "-\\frac{1}{12}(x-5), &3< x\\leq 5\\\\\n",
    "\\frac{1}{6}(x-5), &5< x\\leq 7\\\\\n",
    "-\\frac{1}{6}(x-9), &7< x\\leq 9\\\\\n",
    "0, &otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Haven't we made it clear?*  We were also introduced to $h$ the following function of $X$:\n",
    "\n",
    "$$h(X) = \\frac{1}{3\\sqrt{2}\\pi}\\mathrm{exp}\\left\\{ -\\frac{1}{18}\\left( X - 5\\right)^2\\right\\}$$\n",
    "\n",
    "\n",
    "*Want us to spell it out for you?* Compute $\\mathbb{E}[h(X)]$ via Monte Carlo simulation using the following sampling methods:\n",
    "\n",
    "**1.1.** Rejection sampling with a normal proposal distribution and appropriately chosen parameters (aka rejection on steroids)\n",
    "\n",
    "**1.2.** Importance sampling with a uniform proposal distribution\n",
    "\n",
    "**1.3.** Importance sampling with a normal proposal distribution and appropriately chosen parameters\n",
    "\n",
    "--\n",
    "\n",
    "**1.4.** So far (in HWs 5 and 6) we've computed estimates of $\\mathbb{E}[h(X)]$ for the following list of methods:\n",
    "\n",
    "* Inverse Transform Sampling\n",
    "* Rejection Sampling with a uniform proposal distribution (rejection sampling in a rectangular box with uniform probability of sampling any x)\n",
    "* Rejection sampling with a normal proposal distribution and appropriately chosen parameters (aka rejection on steroids)\n",
    "* Importance sampling with a uniform proposal distribution\n",
    "* Importance sampling with a normal proposal distribution and appropriately chosen parameters.\n",
    "\n",
    "Compute the variance of each estimate of $\\mathbb{E}[h(X)]$ you calculated in this list. Which sampling methods and associated proposal distributions would you expect based on discussions from lecture to have resulted in lower variances? How well do your results align with these expectations?\n",
    "\n",
    "**Gratuitous Titular Reference**:  Annemarie and Marshmello's [F-R-I-E-N-D-S](https://www.youtube.com/watch?v=CY8E6N5Nzec) samples from French Hip Hop artist Rapsa's [42 Mesures à ta Sa(i)nté](https://www.youtube.com/watch?v=JivaoPSAgLI) a homage to his hometown of Saint-Étienne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Mr. Poe Writes of Gradient Descent Into the Maelström`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are building a pricing model for laying down telecom cables over a geographical region. You construct a pricing model that takes as input a pair of coordinates, $(x_1, x_2)$ and based upon two parameters $\\lambda_1, \\lambda_2$ predicts the loss in revenue corresponding to laying the cables at the inputed location.  Your pricing model is described by the following equation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x_1, x_2\\ \\vert\\ \\lambda_1, \\lambda_2) = 0.000045\\lambda_2^2 y - 0.000098\\lambda_1^2 x_1 + 0.003926\\lambda_1 x_1\\exp\\left\\{\\left(x_2^2 - x_1^2\\right)\\left(\\lambda_1^2 + \\lambda_2^2\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "We've provided you some data contained in the file `HW6_data.csv`. This data represents a set of coordinates configured on the curve $x_2^2 - x_1^2 = -0.1$. Your general goal for this problem is to find the parameters $\\lambda_1, \\lambda_2$ that  minimize the net loss over the entire dataset.\n",
    "\n",
    "**2.1.** Construct an appropriate visualization of the loss function for the given data.  Use that visualization to verify that for $\\lambda_1 = 2.05384, \\lambda_2 = 0$, the loss function $L$ is minimized.  Your visualization should make note of this optima.\n",
    "\n",
    "**2.2.** Choose an appropriate learning rate  from [10, 1, 0.1, 0.01, 0.001, 0.0001] and use that learning rate to implement gradient descent.  Use your implementation to minimize $L$ for the given data.  Your implementation should be stored in a function named `gradient_descent`.  `gradient_descent` should take the following parameters (n represents the number of data points):\n",
    "\n",
    "- `lambda_init` -- a numpy array with shape (2 , 1) containing the initial value for $\\lambda_1$ and $\\lambda_2$\n",
    "- `X_data` -- an numpy array with shape (n, 2) containing the data coordinates used in your loss function\n",
    "- `step_size` -- a float containing the step-size/learning rate used in your algorithm\n",
    "- `scale` -- a float containing the factor by which you'll scale your step_size (or alternatively your loss) in the algorithm\n",
    "- `max_iterations` -- an integer containing a cap on the number of iterations for which you'll let your algorithm run\n",
    "- `precision` -- a float containing the difference in loss between consecutive iterations below which you'll stop the algorithm\n",
    "- `loss` -- a function (or lambda function) that takes in the following parameters and returns a float with the results of calculating the loss function for our data at $\\lambda_1$ and $\\lambda_2$\n",
    "    - `lambdas` -- a numpy array with shape (2, 1) containing $\\lambda_1$ and $\\lambda_2$\n",
    "    - `X_data` -- the same as the parameter `X_data` for `gradient_descent`\n",
    "    \n",
    "The return value for `gradient_descent` should be a dictionary with the following keys (n_itertions represents the total number of iterations):\n",
    "- 'lambdas' -- the associated value is a numpy array with shape (2,1) containing the optimal $\\lambda$'s found by the algorithm\n",
    "- 'history' -- the associated value is a numpy array with shape (n_iterations,) containing a history of the calculated value of the loss function at each iteration\n",
    "\n",
    "\n",
    "**2.3** For your implementation in 2.2, create a plot of loss vs iteration.  Does your descent algorithm comverge to the right values of $\\lambda$?  At what point does your implementation converge?\n",
    "\n",
    "**2.4.** Choose an appropriate learning rate  from [10, 1, 0.1, 0.01, 0.001, 0.0001] and use that learning rate to implement stochastic gradient descent.  Use your implementation to minimize $L$ for the given data.  Your implementation should a stored in a function named `stochastic_gradient_descent`.  `stochastic_gradient_descent` should take the following parameters (n represents the number of data points):\n",
    "\n",
    "- `lambda_init` -- a numpy array with shape (2 , 1) containing the initial value for $\\lambda_1$ and $\\lambda_2$\n",
    "- `X_data` -- an numpy array with shape (n, 2) containing the data coordinates for your loss function\n",
    "- `step_size` -- a float containing the step-size/learning rate used in your algorithm\n",
    "- `scale` -- a float containing the factor by which you'll scale your step_size (or alternatively your loss) in the algorithm\n",
    "- `max_iterations` -- an integer containing a cap on the number of iterations for which you'll let your algorithm run\n",
    "- `precision` -- a float containing the difference in loss between consecutive iterations below which you'll stop the algorithm\n",
    "- `loss` -- a function (or lambda function) that takes in the following parameters and returns a float with the results of calculating the loss function for our data at $\\lambda_1$ and $\\lambda_2$\n",
    "    - `lambdas` -- a numpy array with shape (2, 1) containing $\\lambda_1$ and $\\lambda_2$\n",
    "    - `X_data` -- the same as the parameter `X_data` for `stochastic_gradient_descent`\n",
    "    \n",
    "The return value for `stochastic_gradient_descent` should be a dictionary with the following keys (n_itertions represents the total number of iterations):\n",
    "- 'lambdas' -- the associated value is a numpy array with shape (2,1) containing the optimal $\\lambda$'s found by the algorithm\n",
    "- 'history' -- the associated value is a numpy array with shape (n_iterations,) containing a history of the calculated value of the loss function at each iteration\n",
    "\n",
    "\n",
    "**2.5** For your implementation in 2.4, create a plot of loss vs iteration.  Does your descent algorithm comverge to the right values of $\\lambda$?  At what point does your implementation converge?\n",
    "\n",
    "**2.6** Compare the average time it takes to update the parameter estimation in each iteration of the two implementations. Which method is faster? Briefly explain why this result should be expected.\n",
    "\n",
    "**2.7** Compare the number of iterations it takes for each algorithm to obtain an estimate accurate to `1e-3`.  You may wish to set a cap for maximum number of iterations.  Which method converges to the optimal point in fewer iterations?  Briefly explain why this result should be expected.\n",
    "\n",
    "**2.8** Compare the performance of stochastic gradient descent on our loss function and dataset for the following learning rates: [10, 1, 0.1, 0.01, 0.001, 0.0001]. Based on your observations, briefly describe the effect of the choice of learning rate on the performance of the algorithm.\n",
    "\n",
    "**2.9** Using your implementation of gradient descent and stochastic gradient descent, document the behavior of your two algorithms for the following starting points, and for a number of stepsizes of your choice:\n",
    "\n",
    "- $(\\lambda_1, \\lambda_2) = (-2.47865, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-3, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-5, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-10, 0)$\n",
    "\n",
    "Construct a mathematical analysis of the loss function $\\mathcal{L}$ to explain results of your descent algorithms at different starting points.\n",
    "\n",
    "**Gratuitous Titular Reference**:  The renowned American auteur Edgar Allen Poe penned [\"A Descent into the Maelström\"](https://en.wikipedia.org/wiki/A_Descent_into_the_Maelstr%C3%B6m), a macabre tale of a seemingly elderly man's surviving a previous horrendous encounter with a massive hurricane, a shipwreck and a sea vortex, in 1841."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
