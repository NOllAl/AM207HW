{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Saturday, October 27th, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "** Place the name of everyone who's submitting this assignment here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "## Standard boilerplate to import torch and torch related modules\n",
    "import torch\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Mon pays c'est l'MNIST. Mon cœur est brise de Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*MNIST* dataset](https://en.wikipedia.org/wiki/MNIST_database) is one of the classic datasets in Machine Learning and is often one of the first datasets against which new classification algorithms test themselves.  It consists of 70,000 images of handwritten digits, each of which is 28x28 pixels. You will be using PyTorch to build a handwritten digit classifier that you will train, validate, and test with MNIST. \n",
    "\n",
    "Your classifier MUST implement a multinomial logistic regression model (using softmax). It will take as input an array of pixel values in an image and output the images most likely digit label (i.e. 0-9). You should think of the pixel values as features of the input vector.  \n",
    "\n",
    "Using the softmax formulation, your PyTorch model should computes the cost function using an L2 regularization approach (see `optim.SGD` in PyTorch or write your own cost function) and minimize the resulting cost function using mini-batch stochastic gradient descent.  We provided  extensive template code in lab.\n",
    "\n",
    "Construct and train your classifier using a batch size of 256 examples, a learning rate $\\eta$=0.1, and a regularization factor $\\lambda$=0.01.\n",
    "\n",
    "1.1. Plot 10 sample images from the MNIST dataset (to develop intuition for the feature space).\n",
    "\n",
    "1.2. Currently the MNIST dataset in Torchvision allows a Train/Test split.  Use PyTorch dataloader functionality to create a Train/Validate/Test split  of 50K/10K/10K samples.\n",
    "\n",
    "**Hint:** Lab described a way to do it keeping within the MNIST `DataLoader` workflow: the key is to pass a `SubsetRandomSampler` to `DataLoader`\n",
    "\n",
    "1.3. Construct a softmax formulation in PyTorch of multinomial logistic regression with Cross Entropy Loss.\n",
    "\n",
    "1.4. Train your model using SGD to minimize the cost function. Use as many epochs as you need to achive convergence.\n",
    "\n",
    "1.5. Plot the cross-entropy loss on the training set as a function of iteration.\n",
    "\n",
    "1.6. Using classification accuracy, evaluate how well your model is performing on the validation set at the end of each epoch. Plot this validation accuracy as the model trains.\n",
    "\n",
    "1.6. Duplicate this plot for some other values of the regularization parameter $\\lambda$. When should you stop the training for each of the different values of λ? Give an approximate answer supported by using the plots.\n",
    "\n",
    "1.7. Select what you consider the best regularization parameter and predict the labels of the test set. Compare your predictions with the given labels. What classification accuracy do you obtain on the training and test sets?\n",
    "\n",
    "1.8. What classes are most likely to be misclassified? Plot some misclassified training and test set images.\n",
    "\n",
    "**Gratuitous Titular Reference**:  The recently departed French rockstar Johnny Hallyday just posthumously released what looks to be his biggest album ever \"Mon pays c'est l'amour\".  The album sold 300,000 copies on its first day of release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.1. Plot 10 sample images from the MNIST dataset (to develop intuition for the feature space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot 10 samples from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and transform train dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3, 4, 1, 7, 5, 0, 0, 7, 8, 5, 7, 4, 4, 6, 5, 9, 6, 4, 4, 7, 2, 8, 5,\n",
      "        3, 0, 7, 8, 3, 3, 6, 0])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-c974ee6f5cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
     ]
    }
   ],
   "source": [
    "images, labels = next(train_loader)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10507ec50>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c802e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0243)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABWAAAACUCAYAAAAdzIB1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+f9c77O54kgE5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ca6d080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(24, 24))\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(1, len(images), i+1)\n",
    "    plt.imshow(images[i, 0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.2. Currently the MNIST dataset in Torchvision allows a Train/Test split.  Use PyTorch dataloader functionality to create a Train/Validate/Test split  of 50K/10K/10K samples.\n",
    "\n",
    "> **Hint:** Lab described a way to do it keeping within the MNIST `DataLoader` workflow: the key is to pass a `SubsetRandomSampler` to `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `random_split` instead of `SubsetRandomSampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST('../mnist_data',\n",
    "                             train=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                 transforms.Normalize((0.5,), (0.5,)) # normalize inputs\n",
    "                                 ]))\n",
    "\n",
    "train, valid = torch.utils.data.random_split(mnist_train, lengths=[50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=32)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=32)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../mnist_data',\n",
    "                   train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                       ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.3. Construct a softmax formulation in PyTorch of multinomial logistic regression with Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc = torch.nn.Linear(784, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        x = self.fc(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.4. Train your model using SGD to minimize the cost function. Use as many epochs as you need to achive convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30...  Loss: 0.4050 Accuracy: 0.8859\n",
      "Epoch: 1/30...  Loss: 0.4032 Accuracy: 1.7715\n",
      "Epoch: 1/30...  Loss: 0.3944 Accuracy: 2.6605\n",
      "Epoch: 2/30...  Loss: 0.0379 Accuracy: 0.0830\n",
      "Epoch: 2/30...  Loss: 0.3815 Accuracy: 0.9731\n",
      "Epoch: 2/30...  Loss: 0.3867 Accuracy: 1.8618\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-4c5d3ad00356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrunning_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Flatten MNIST images into a 784 long vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "print_every = 400\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    for images, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward and backward passes\n",
    "        output = model.forward(images)\n",
    "        #print(output.size())\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "            \n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: MNIST MLP!  Find out what that means to me.  MNIST MLP!  Take care, TCB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multilayer perceptron can be understood as a logistic regression classifier in which the input is first transformed using a learnt non-linear transformation. The non-linear transformation is often chosen to be either the logistic function or the $\\tanh$ function or the RELU function, and its purpose is to project the data into a space where it becomes linearly separable. The output of this so-called hidden layer is then passed to the logistic regression graph that we have constructed in the first problem. \n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/mlp.png)\n",
    "\n",
    "We'll construct a model with **1 hidden layer**. That is, you will have an input layer, then a hidden layer with the nonlinearity, and finally an output layer with cross-entropy loss (or equivalently log-softmax activation with a negative log likelihood loss).\n",
    "\n",
    "2.1. Using a similar architecture as in Question 1 and the same training, validation and test sets, build a PyTorch model for the multilayer perceptron. Use the $\\tanh$ function as the non-linear activation function. \n",
    "\n",
    "2.2. The initialization of the weights matrix for the hidden layer must assure that the units (neurons) of the perceptron operate in a regime where information gets propagated. For the $\\tanh$ function, you may find it advisable to initialize with the interval $\\left[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right]$, where $fan_{in}$ is the number of units in the $(i-1)$-th layer, and $fan_{out}$ is the number of units in the i-th layer.  This is known as **Xavier Initialization**.  Use Xavier Initialization to initialize your MLP.  Feel free to use PyTorch's in-built Xavier Initialization methods.\n",
    "\n",
    "2.3. Using $\\lambda = 0.01$ to compare with Question 1, experiment with the learning rate (try 0.1 and 0.01 for example), batch size (use 64, 128 and 256) and the number of units in your hidden layer (use between 25 and 200 units). For what combination of these parameters do you obtain the highest validation accuracy?  You may want to start with 20 epochs for running time and experiment a bit to make sure that your models reach convergence. \n",
    "\n",
    "2.4. For your best combination plot the cross-entropy loss on the training set as a function of iteration.\n",
    "\n",
    "2.5. For your best combination use classification accuracy to evaluate how well your model is performing on the validation set at the end of each epoch. Plot this validation accuracy as the model trains.\n",
    "\n",
    "2.6. Select what you consider the best set of parameters and predict the labels of the test set. Compare your predictions with the given labels. What classification accuracy do you obtain on the training and test sets?\n",
    "\n",
    "2.7. How does your test accuracy compare to that of the logistic regression classifier in Question 1?  Compare best parameters for both models.\n",
    "\n",
    "2.8. What classes are most likely to be misclassified? Plot some misclassified training and test set images.\n",
    "\n",
    "\n",
    "**Gratuitous Titular Reference**:  Respect, originally performed by Otis Redding, became a huge hit and an anthem for the recently departed \"Queen of Soul\" Aretha Franklin.  Respect is often credited with popularizing the word usages \"propers\" (a synonym for respect) and \"sock it to me\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
