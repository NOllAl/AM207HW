{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El Fin (Final Exam) for AMPTH-2017/APMA E-207\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date:** Monday, December 17th, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "** Place the name of everyone who's submitting this assignment here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn.apionly as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: GLMs with correlation\n",
    "\n",
    "### The dataset: A Bangladesh Contraception use census\n",
    "\n",
    "This problem is based on one-two (12H1 and continuations ) from your textbook. The data is in the file `bangladesh.csv`. These data are from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables:\n",
    "\n",
    "- (1) `district`: ID number of administrative district each woman resided in\n",
    "- (2) `use.contraception`: An indicator (0/1) of whether the woman was using contraception\n",
    "- (3) `urban`: An indicator (0/1) of whether the woman lived in a city, as opposed to living in a rural area\n",
    "- (4) `woman`: a number indexing a single woman in this survey\n",
    "- (5) `living.chidren`: the number of children living with a woman\n",
    "- (6) `age.centered`: a continuous variable representing the age of the woman with the sample mean subtracted\n",
    "\n",
    "We need to make sure that the cluster variable, district, is a contiguous set of integers, so that we can use the index to differentiate the districts easily while sampling ((look at the Chimpanzee models we did in lab to understand the indexing). So create a new contiguous integer index to represent the districts. Give it a new column in the dataframe, such as `district.id`.\n",
    "\n",
    "You will be investigating the dependence of contraception use on the district in which the survey was done. Specifically, we will want to regularize estimates from those districts where very few women were surveyed. We will further want to investigate whether the areas of residence (urban or rural) within a district impacts a woman's use of contraception.\n",
    "\n",
    "Feel free to indulge in any exploratory visualization which helps you understand the dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "We will use `use.contraception` as a Bernoulli response variable. \n",
    "\n",
    "When we say \"fit\" below, we mean, specify the model, plot its graph, sample from it, do some tests, and forest-plot and summarize the posteriors, at the very least.\n",
    "\n",
    "**A1** Fit a traditional \"fixed-effects\" model which sets up district-specific intercepts, each with its own Normal(0, 10) prior. That is, the intercept is modeled something like \n",
    "\n",
    "```python\n",
    "alpha_district = pm.Normal('alpha_district', 0, 10, shape=num_districts)\n",
    "p=pm.math.invlogit(alpha_district[df.district_id])\n",
    "```\n",
    "\n",
    "Why should there not be any overall intercept in this model? \n",
    "\n",
    "**A2** Fit a multi-level \"varying-effects\" model with an overall intercept `alpha`, and district-specific intercepts `alpha_district`. Assume that the overall intercept has a Normal(0, 10) prior, while the district specific intercepts are all drawn from the **same** normal distribution with mean 0 and standard deviation $\\sigma$. Let $\\sigma$ be drawn from HalfCauchy(2). The setup of this model is similar to the per-chimanzee models in the prosocial chimanzee labs.\n",
    "\n",
    "**A3** What does a posterior-predictive sample in this model look like? What is the difference between district specific posterior predictives and woman specific posterior predictives. In other words, how might you model the posterior predictive for a new woman being from a particular district vs that os a new woman in the entire sample? This is a word answer; no programming required.\n",
    "\n",
    "**A4** Plot the predicted proportions of women in each district using contraception against the id of the district, in both models. How do these models disagree? Look at the extreme values of predicted contraceptive use in the fixed effects model. How is the disagreement in these cases?\n",
    "\n",
    "**A5** Plot the absolute value of the difference in probability of contraceptive use against the number of women sampled in each district. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B.\n",
    "\n",
    "Let us now fit a model with both varying intercepts by `district_id` (like we did in the varying effects model above) and varying slopes of `urban` by `district_id`. To do this, we will \n",
    "\n",
    "- (a) have an overall intercept, call it `alpha`\n",
    "- (b) have an overall slope of `urban`, call it `beta`.\n",
    "- (c) have district specific intercepts `alpha_district`\n",
    "- (d) district specific slopes for `urban`, `beta_district`\n",
    "- (e) model the co-relation between these slopes and intercepts. \n",
    "\n",
    "We have not modelled covariance and correlation before, so look at http://am207.info/wiki/corr.html for notes on how this is done.\n",
    "\n",
    "To see the ideas behind this, see section 13.2.2 on the income data  from your textbook (included as a pdf in this zip). Feel free to use [code with attribution from Osvaldo Martin](https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3/blob/master/Chp_13.ipynb)..with attribution and understanding...there is some sweet pymc3 technical wrangling in there.\n",
    "\n",
    "**B1** Write down the model as a pymc3 specification and look at its graph. Note that this model builds a 60 by 2 matrix with `alpha_district` values in the first column and `beta_district` values in the second. By assumption, the first column and the second column have correlation structure given by an LKJ prior, but there is no explicit correlation among the rows. In other words, the correlation matrix is 2x2 (not 60x60). Make sure to obtain the value of the off-diagonal correlation as a `pm.Deterministic`. (See Osvaldo Martin's code above)\n",
    "\n",
    "**B2**: Sample from the posterior of the model above *with a target acceptance rate of .9 or more*. (Sampling takes me 7 minutes 30 seconds on my 2013 Macbook Air). Comment on the quality of the samples obtained.\n",
    "\n",
    "**B3** Propose a method based on the reparametrization trick for multi-variate gaussians) of improving the quality of the samples obtained and implement it. (A hint can be obtained from here: https://docs.pymc.io/api/distributions/multivariate.html#pymc3.distributions.multivariate.MvNormal . Using that hint lowered the sampling time to 2.5 minutes on my laptop).\n",
    "\n",
    "**B4** Inspect the trace of the correlation between the intercepts and slopes, plotting the correlation marginal. What does this correlation tell you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Also plot the predicted proportion of women using contraception, with urban women on one axis and rural on the other.  Finally, also plot the difference between urban and rural probabilities against rural probabilities. All of these will help you interpret your findings. (Hint: think in terms of low or high rural contraceptive use)\n",
    "\n",
    "**B5** Add additional \"slope\" terms (one-by-one) into the model for \n",
    "\n",
    "- (a) the centered-age of the women and \n",
    "- (b) an indicator for whether the women have a small number or large number of existing kids in the house (you can treat 1-2 kids as low, 3-4 as high, but you might want to experiment with this split). \n",
    "\n",
    "Are any of these effects significant? Are any significant effects similar over the urban/rural divide?\n",
    "\n",
    "**B6** Use WAIC to compare your models. What are your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Mixture of experts and mixture density networks to solve inverse problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you had to predict a one-to-many function? The data provided below comes from a dataset generated by Chris Bishop (yes that Bishop) to explain the models mentioned in the title above. We have included pdfs from his book which describe these models in some detail. We saw this model earlier in HW where we did an EM like algorithm to obtain a mixture of regressions.\n",
    "\n",
    "The data is in `one-to-many.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the data it looks like this. Notice both the uneven sampling (more towards the center), and the \"more than one y\" for a given x.\n",
    "\n",
    "![](images/inverse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal regression approaches to modeling such a function wont work, as they expect the function to be a proper mathematical function, that is, single valued.\n",
    "\n",
    "These kind of problems are called **inverse problems**, where more than one input state leads to an output state, and we have to try and model these multiple input states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mixture of gaussians (or other distributions) might is a sensible way to do this.\n",
    "\n",
    "You choose one of the gaussians with some probability. The nean of the gaussian is then given by some regression function, say for example a straight line. We could additionally fix the standard deviation or model it as well. \n",
    "\n",
    "Thus, for each component Gaussian, we choose a functional form for the mean and standard deviation. So our model looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x)  = \\sum_i \\lambda_i g_i (x) $$\n",
    "\n",
    "Say we fit a model with 3 gaussians to this data. Such a model cannot fit the function above. Notice for example that at $x=0.2$ only one of the gaussians will dominate, different from the situation at $x=0.5$. This means that the probabilities of \"belonging\" to one or the other gaussians is also changing with $x$.\n",
    "\n",
    "If we allow the mixing probabilities to depend on $x$, we can model this situation.\n",
    "\n",
    "$$f(x)  = \\sum_i \\lambda_i (x) g_i (x) $$\n",
    "\n",
    "Such a model is called a \"mixture of experts\" model. The idea is that one \"expert\" gaussian is responsible in one sector of the feature space, while another expert is responsible in another sector.\n",
    "\n",
    "You can think of this model as implementing a \"standard\" gaussian mixture at each \"point\" x, with the added complexity that all of the means, standard deviations, and mixture probabilities change from one x to another.\n",
    "\n",
    "See https://www.cs.toronto.edu/~hinton/absps/hme.pdf and http://www.ee.hacettepe.edu.tr/~eyuksel/Publications/2012_TwentyYearsofMixtureofExperts.pdf for more details. I found the latter clearer and easier to understand.\n",
    "\n",
    "For this entire question you might find diagram code from [here](https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb) useful. Take with attribution.\n",
    "\n",
    "We will assume we have **3 gaussians**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Variational Mixture of experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll construct a gaussian mixture model of 3 \"expert\" linear regressions. The idea is to create a fit which looks like this:\n",
    "\n",
    "![](images/mixreg.png)\n",
    "\n",
    "Here the three regression lines work in different regions of $f$. We want a pricipled way to sample from this model and to be able to produce posteriors and posterior-predictives.\n",
    "\n",
    "There are 3 parts to this model. First the means of the gaussians in the mixture are modeled with linear regression as shown in the picture above. We will also model $log(\\sigma)$ for each gaussian in the mixture as a linear regression as well ($\\sigma$ needs to be positive).\n",
    "\n",
    "We now need to model the mixture probabilities, i.e., the probabilities required to choose one or the other gaussian. These mixing probabilities, the $\\lambda$s will be modeled as a softmax regression (ie do a linear regression and softmax it to get 3 probabilities).\n",
    "\n",
    "**A1** Write a pymc3 model for this problem. For all biases and weights in your regressions, assume N(0,5) priors. Add noise 0.01 to each of the three $\\sigma$s to make sure you dont have a collapsed 0 width gaussian, ie we want some data in every cluster. (Thus to get the final $\\sigma$, you will exponentiate your regression for $log(\\sigma)$ and add 0.01.)\n",
    "\n",
    "**A2** Fit this model variationally for about 50,000 iterations using the adam optimizer. (`obj_optimizer=pm.adam()`) Plot the ELBO to make sure you have converged. Print summaries and traceplots for the means, $\\sigma$s and probabilities.\n",
    "\n",
    "**A3** Plot the mean posteriors with standard deviations against x. Also produce a diagram like the one above to show the mean\"s with standard deviations showing their uncertainty overlaid on the data.\n",
    "\n",
    "**A4** Plot the posterior predictive (mean and variance) as a function of x) for this model (using `sample_ppc` for example). Why does the posterior predictive look nothing like the data?\n",
    "\n",
    "**A5** Make a \"correct\" posterior predictive diagram by taking into account which \"cluster\" or \"regression line\" the data is coming from. To do this you will need to sample using the softmax probabilities. A nice way to do this is \"Gumbel softmax sampling\". See http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/ for details. Color-code the predictive samples with the gaussian they came from. Superimpose the predictive on the original data. You may want to contrast a prediction from a point estimate at the mean values of the $\\mu$ and $\\sigma$ traces at a given x (given the picked gaussian) to the \"full\" posterior predictive obtained from sampling from the entire trace of $\\mu$ and $\\sigma$ and $\\lambda$. The former diagram may look something like this:\n",
    "\n",
    "![](images/mixpred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Mixture Density Network\n",
    "\n",
    "A mixture density network (see the enclosed Chapter 5 excerpt from Bishop or https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf) is very closely related to the mixture of experts model. The difference is that we fit the regressions using a neural network where hidden layers are shared amongst the mean, sigma, and mixing probability regressions. (We could have fit 3 separate neural networks in Part A but opted to fit linear regressions for simplicity)\n",
    "\n",
    "(More explanation [here](https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb). You are welcome to take code from here with attribution.)\n",
    "\n",
    "You job here is to construct a multi-layer perceptron model with a linear hidden layer with 20 units followed by a `Tanh` activation. After the activation layer, 3 separate linear layers with `n_hidden` inputs and `n_gaussian=3` outputs will complete the network. The probabilities part of the network is then passed through a softmax. The means part is left as is. The sigma part is exponentiated and 0.01 added, as in part A\n",
    "\n",
    "Thus the structure looks like:\n",
    "\n",
    "```\n",
    "input:1 --linear-> n_hidden -> Tanh --linear-->n_gaussians      ...mu\n",
    "                            --linear-->n_gaussians->softmax     ...lambda\n",
    "                            --linear-->n_gaussians->exp + 0.01  ...sigma\n",
    "```\n",
    "\n",
    "We then need to use a loss function for the last layer of the network. \n",
    "\n",
    "Using the mean-squared-error loss is not appropriate as the expected value of samples drawn from the sampling distribution of the network will not reflect the 3-gaussian structure (this is the essence of the difference between A4 and A5 above). Thus we'll use the negative loss likelihood of the gaussian mixture model explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B1**: Write the network as a class `MixtureDensityNetwork` which inherits from pytorch `nn.Module`. Implement a constructor which allows at-least the number of hidden layers to be varied. Also implement the `forward` method.\n",
    "\n",
    "**B2**: Train the network using the Adam or similiar optimizer and gradient descent/SGD. Make sure your loss converges and plot this convergence.\n",
    "\n",
    "**B3**: Plot the MLE parameters against x. Make a plot similar to A3 above where you overlay the \"means\" of the gaussians against the data.  Plot traces of the mu/sigma/lambda as an aid in debugging.\n",
    "\n",
    "**B4**: Sample from the sampling distributions at the estimated point values of $\\mu$ and $\\sigma$ (given cluster) to make a plot similar to A5 above\n",
    "\n",
    "**To think but not to hand in** What are the differences between a mixture density network and the mixture of experts. How do these differences translate to feature space? What would happen if we took the shared hidden layer nonlinearity (Tanh) out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Variational Mixture Density Network\n",
    "\n",
    "We want to implement the Mixture Density Metwork model that we constructed in Part B  directly in pymc3 and use variational inference to sample from it. We  may need more iterations in order to get convergence as this model will likely not converge as fast as the pytorch equivalent.\n",
    "\n",
    "**C1**: Write out the equivalent pymc3 version of the MDN and generate posterior samples with ADVI.\n",
    "\n",
    "**C2**: Sample from the posterior predictive and produce a diagram like B4 and A5 for this model. Plot traces of the mu/sigma/lambda as an aid in debugging your sampler.\n",
    "\n",
    "**C3**: Plot the \"mean\" regression curves (similar to B3 and A3). Do the \"mean\" regression curves in this model look the same from those in Part B?  If they differ why so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3:  Exploring Temperature in Sampling and Optimiztion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At various times in class we've discussed in very vague terms the relation between \"temperature\" and sampling from or finding optima of distributions.  Promises would invariably be made that at some later point we'd discuss the concept of temperature and sampling/optima finding in more detail.  Let's take this problem as an opportunity to keep our promise.\n",
    "\n",
    "Let's start by considering the function $f(x, y)$ defined in the following code cell. $f(x, y)$ is a mixture of three well separated Gaussian probability densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_cov = lambda  theta: np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "theta_vec = (5.847707364986893, 5.696776968254305, 1.908095937315489)\n",
    "theta1, theta2, theta3 = theta_vec\n",
    "\n",
    "# define gaussian mixture 1 \n",
    "cov1 = make_cov(theta1)\n",
    "sigma1 = np.array([[2, 0],[0, 1]])\n",
    "mvn1 = scipy.stats.multivariate_normal([12, 7], cov=cov1@sigma1@cov1.T)\n",
    "\n",
    "# define gaussian mixture 2\n",
    "cov2 = make_cov(theta2)\n",
    "sigma2 = np.array([[1, 0],[0, 3]])\n",
    "mvn2 = scipy.stats.multivariate_normal([-1, 6], cov=cov2@sigma2@cov2.T)\n",
    "\n",
    "cov3 = make_cov(theta3)\n",
    "sigma3 = np.array([[.4, 0],[0, 1.3]])\n",
    "mvn3 = scipy.stats.multivariate_normal([3,-2], cov=cov3@sigma3@cov3.T)\n",
    "\n",
    "f = lambda xvec: mvn1.pdf(xvec) + mvn2.pdf(xvec) + .5*mvn3.pdf(xvec)\n",
    "\n",
    "p = lambda x, y: f([x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Visualization and Metropolis\n",
    "\n",
    "**A1**. Visualize $p(x, y)$ with a contour or surface plot.  Make sure to title your plot and label all axes.  What do you notice about $p(x, y)$?  Do you think it will be an easy function to sample?\n",
    "\n",
    "**A2**. Generate 20000 samples from $p(x, y)$ using the Metropolis algorithm.  Pick individual gaussian proposals in $x$ and $y$ with $\\sigma=1$, initial values, burnin parameters, and thinning parameter.  Plot traceplots of the $x$ and $y$ marginals as well as autocorrelation plots.  Plot a pathplot of your samples.  Based on your visualizations, has your Metropolis sampler generated an appropriate representation of the distribution $p(x, y)$?\n",
    "\n",
    "A pathplot is just your samples trace overlaid on your pdf, so that you can see how the sampler traversed. It looks something like this:\n",
    "\n",
    "![](images/pathplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Changing pdfs using temperature\n",
    "\n",
    "Given a function $p(x)$ we can rewrite that function in following way:\n",
    "\n",
    "$$p(x) = e^{-(-\\log(p(x))}$$\n",
    "\n",
    "So if define the energy density for a function as $E(x)\\equiv-\\log{p(x)}$\n",
    "\n",
    "We can now aim to sample from the function parameratized by a Temperature $T$.\n",
    "\n",
    "$$p(x\\vert T) = e^{-\\frac{1}{T} E(x)} = p(x)^{\\frac{1}{T}}$$\n",
    "\n",
    "If we set T=1 we're sampling from our original function $p(x)$. \n",
    "\n",
    "**B1** In line with A1, visualize modified pdfs (dont worry about normalization) by setting the temperatures to $T=10$ and $T=0.1$. \n",
    "\n",
    "**B2**. Modify your Metropolis algorithm above to take a temperature parameter `T` as well as to keep track of the number of rejected proposals.  Generate 20000 samples from $p(x, y)$ at for each of the following temperatures: {0.1, 1, 3, 7, 10}. Construct  histograms of the marginals, traceplots, autocorrelation plots, and a pathplot for your samples at each temperature.  What happens to the number of rejections as temperature increases? In the limits $T \\rightarrow 0$ and $T \\rightarrow \\infty$ what do you think your samplers will do?\n",
    "\n",
    "**B3**. Approximate the $f(X)$ by the appropriate mixture of Gaussians as a way of generating samples from $f(X)$ to compare with other sampling methods.  Use scipy.stats.multivariate_normal to generate 20000 samples.  How do the histograms compare with the histograms for the samples from $f(X)$ at each temperature.  At what temperature do the samples best represent the function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Parallel Tempering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen some of the pproperties of sampling at higher temperatures, let's explore a way to incorporate the improved exploration of the *entire pdf* from sampling at higher temperatures while still getting samples that match our distribution.  We'll use a technique called *parallel tempering*.  \n",
    "\n",
    "The general idea of parallel tempering is to simulate $N$ replicas of the original system of interest (in our case, a single Metropolis Hastings chain), each replica at a different temperature. The temperature of a Metropolis Hastings Markov Chain defines how likely it is to sample from a low-density part of the target distribution. The high temperature systems are generally able to sample large volumes of parameter space, whereas low temperature systems, while having precise sampling in a local region of parameter space, may become trapped around local energy minima/probability maxima. Parallel tempering achieves good sampling by allowing the chains at different temperatures to exchange complete configurations. Thus, the inclusion of higher temperature chains ensures that the lower temperature chains can access *all* the low-temperature regions of phase space: the higher temperatures help these chains make the jump-over.\n",
    "\n",
    "Darren Wilkinson's blog post has a [good description](https://darrenjw.wordpress.com/2013/09/29/parallel-tempering-and-metropolis-coupled-mcmc/) of whats going on.\n",
    "\n",
    "Here is the idea that you must implement.\n",
    "\n",
    "There are $N$ replicas each at different temperatures $T_i$ that produce $n$ samples each before possibly swapping states.\n",
    "\n",
    "We simplify matters by only swapping states at adjacent temperatures.  The probability of swapping any two instances of the replicas is given by\n",
    "\n",
    "$$A = min\\left(1, \\frac{p_k(x_{k+1})p_{k+1}(x_k)}{p_k(x_k) p_{k+1}(x_{k+1})}\\right)$$\n",
    "\n",
    "One of the $T_i$'s in our set will always be 1 and this is the only replica that we use as output of the Parallel tempering algorithm.\n",
    "\n",
    "An algorithm for Parallel Tempering is as follows:\n",
    "\n",
    "1. Initialize the parameters $\\{(x_{init}, y_{init})_i\\}, \\{T_i\\}, L$ where \n",
    "    * $L$ is the number of iterations between temperature swap proposals.\n",
    "    * $\\{T_i\\}$ is a list of temperatures.  You'll run one chain at each temperature.\n",
    "    * $\\{(x_{init}, y_{init})_i\\}$ is a list of starting points, one for each chain \n",
    "2. For each chain (one per temperature) use the simple Metropolis code you wrote earlier. Perform $L$ transitions on each chain.\n",
    "3. Set the $\\{(x_{init}, y_{init})_i\\}$ for the next Metropolis run on each chain to the last sample for each chain i.\n",
    "4. Randomly choose 2 chains at adjacent temperatures.\n",
    "    1. Use the above formula to calculate the Acceptance probability $A$.\n",
    "    2. With probability $A$, swap the positions between the 2 chains (that is swap the $x$s of the two chains, and separately swap the $y$s of the chains .\n",
    "5. Go back to 2 above, and start the next L-step epoch \n",
    "6. Continue until you finish $Num. Samples//L$ epochs.\n",
    "\n",
    "\n",
    "**C1**. Explain why swapping states with the given acceptance probability is in keeping with detailed balance. The linked blog post might help.\n",
    "\n",
    "**C2**. Create a parallel tempering sampler that uses 5 chains at  the temperatures {0.1, 1, 3, 7, 10} to sample from $f(x, y)$.  Choose a value of L around 10-20.  Generate 10000 samples from $f(x, y)$.  Construct  histograms of the marginals, traceplots, autocorrelation plots, and a pathplot for your samples.\n",
    "\n",
    "**C3**. How do your samples in **C2** compare to those of the Metropolis sampler?  How do they compare to the samples generated from the Gaussian Mixture approximation of $f(x, y)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D. Global Optima using Simulated Annealing\n",
    "\n",
    "We have new-found intuition about how to use temperature to improve our sampling. Lets now tackle the inverse idea: what happens if you sample at a lower temperature than 1. Our visualizations from Part B should indicate to us that the distributions become extremely tightly peaked arounnd their maxima.\n",
    "\n",
    "If we initialized a metropolis-hastings sampler around an optimum at a really low temperature, it would find us a local minimum. But if we had a higher temperature at the beginning, we can use Metropolis-Hastings sampling at high temperatures to travel around the distribution and find all the peaks (valleys).  Then we will slowly cool down the temperature (which will allow us to escape local optima at higher temperatures) and finally focus us into a particular optimum region and allow you to find the optimum. It can be shown that for ceratin *temperture schedules* this method is guaranteed to find us a global minimum in the limit of infinite iterations.\n",
    "\n",
    "We'll use this methd to find the global minimum of our distribution. The algorithm is as follows. Now we have only one chain, but we very slowly dial down its temperature to below T=1.\n",
    "\n",
    "1. Initialize $(x, y)_i,T, L(T)$ where $L$ is the number of iterations at a particular temperature. \n",
    "2. Perform $L$ transitions thus(we will call this an epoch):\n",
    "    1. Generate a new proposed position $(x, y)_{\\ast}$ using 2 independent gaussians with $\\sigma=1$.\n",
    "    2. If $(x, y)_{\\ast}$ is accepted (according to probability $P = e^{(-\\Delta E/T)}$, set $(x, y)_{i+1} = (x, y)_{\\ast}$, else set $(x, y)_{i+1} = x_{i}$  \n",
    "3. Update T and L \n",
    "4. Until some fixed number of epochs, or until some stop criterion is fulfilled, goto 2.\n",
    "\n",
    "$\\Delta E$ is the change in enery, or the change in the negative log of the probability function. That is, $E = -log p(x,y)$. For a given T and L, this is just Metropolis!\n",
    "\n",
    "This algorithm is called *simulated annealing* and we'll use it to find the global maximum for $f(X)$\n",
    "\n",
    "**D1**. Use simulated annealing with a cooling schedule of $T_{k+1}=0.98T_{k}$ and a L(T) defined initially at 100 with $L_{k+1} = 1.2 L_k$ to find the global optima for $p(x, y)$.  Plot $E(x, y)$ vs iterations.  Given how we constructed $p(x, y)$ it should be fairly straight-forward to observe the  true optima by inspection.  How does the optima found by SA compare to the true optima?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
