{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "In this tutorial we introduce the \"Bayes by Backprop\" algorithm. It is an algorithm to make the calculation of the posterior weight distribution of neural networks tractable. The main idea of this method is to combine two standard methods, one from the Bayesian real and one from the frequentist realm:\n",
    "\n",
    "1. Variation inference: this is a standard technique to approximate the posterior distribution in situations that are otherwise intractable. The fundamental idea is very simple: we can't do the posterior calculations (or sampling, for that matter) exactly, so we approximate the posterior with distributions we know.\n",
    "2. Backpropagation (a.k.a. gradient descent): this is the default technique to optimize the weights in neural networks.\n",
    "\n",
    "It is shown that we can use backpropagation to perform variational inference.\n",
    "\n",
    "We use \"Bayes by Backprop\" on three different datasets:\n",
    "\n",
    "1. A simulated dataset for a classification task with different Bayes rate in different regions.\n",
    "2. MNIST: here we reproduce the results from the original paper and extend the method to convolutional neural networks (*at least, we try?*)\n",
    "3. TBD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## What are Bayesian neural networks\n",
    "\n",
    "## Why Bayesian nets\n",
    "\n",
    "## Problems with naive approach (sampling)\n",
    "\n",
    "## Solution with Variation inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference\n",
    "\n",
    "## What is variational inference?\n",
    "\n",
    "## Easy example (maybe Gaussian and Gaussian mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes by backprop\n",
    "\n",
    "## Derivation\n",
    "\n",
    "## Variational posterior\n",
    "\n",
    "## Description of prior\n",
    "\n",
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application and Implementation\n",
    "\n",
    "## Simulated dataset\n",
    "\n",
    "Possibly binary classification with different best achievable accuracy in different regions.\n",
    "\n",
    "## MNIST\n",
    "\n",
    "Could try to extend algorithm to CNN. Should not be too hard (don't blame me for that sentence later)\n",
    "\n",
    "## Real life dataset\n",
    "\n",
    "(Maybe claims frequency prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html\n",
    "+ https://arxiv.org/pdf/1505.05424.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
